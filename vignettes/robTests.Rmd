---
title: "Getting started with `robTests`"
date: "2021-04-25"
author: Sermad Abbas, Barbara Brune, Roland Fried 
output: 
  rmarkdown::html_vignette:
    toc: true
bibliography: ../inst/REFERENCES.bib
csl: csda.csl
vignette: >
  %\VignetteIndexEntry{robTests}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



# Introduction {#introduction}

The package `robTests` contains robust (approximately) distribution-free tests for the two-sample location problem.

We consider the following data situation: Let $\boldsymbol{X} = \left(X_1, \ldots, X_m\right)$ and $\boldsymbol{Y} = \left(Y_1, \ldots, Y_n\right)$ be two samples of sizes $m, n \in \mathbb{N}$.
Each sample consists of independent and identically distributed (i.i.d.) random variables so that

\begin{align*}
  X_1, \ldots, X_m \overset{i.i.d.}{\sim} F_X \quad \text{and} \quad Y_1, \ldots, Y_n \overset{i.i.d.}{\sim} F_Y,
\end{align*}

where $F_X, F_Y\colon\ \mathbb{R} \to \left[0, 1\right]$ are the CDFs of the underlying unknown continuous distributions. Let $G$ be a shifted version of $F$, i.e.

\begin{align*}
  F_Y\left(x\right) = F\left(x + \Delta\right) \quad \text{for all}\ x \in \mathbb{R},
\end{align*}

with $\Delta \in \mathbb{R}$ denoting the shift size. Hence, $F$ and $G$ differ at most in location and, in particular, have the same variance $\sigma^2 := \sigma_X^2 = \sigma_Y^2 > 0$ (homoscedasticity).

The following hypotheses can be tested:
\begin{align*}
  &H_0^{(=)}\colon\ \Delta = \Delta_0 \quad \text{vs.} \quad H_1^{(=)}\colon\ \Delta \neq \Delta_0 \\
  &H_0^{(\leq)}\colon\ \Delta \leq \Delta_0 \quad \text{vs.} \quad H_1^{(\leq)}\colon\ \Delta > \Delta_0 \\
  &H_0^{(\geq)}\colon\ \Delta \geq \Delta_0 \quad \text{vs.} \quad H_1^{(\geq)}\colon\ \Delta < \Delta_0,
\end{align*}
where $\Delta_0 \in \mathbb{R}$ is a value to which $\Delta$ is set into relation.

A popular test for this scenario is the ordinary two-sample $t$-test, which uses the normality assumption. For large samples, the central limit theorem ensures that the test keeps the desired significance level $\alpha \in \left(0, 1\right)$, even if the underlying distributions are not normal. However, relying on the central limit theorem is often not appropriate in small samples. 
Moreover, the test is known to be vulnerable to outlying values as they can mask existing location shifts or lead to wrong rejections of the null hypothesis. 

The Wilcoxon-Mann-Whitney test is a popular distribution-free test which is nearly as powerful as the $t$-test under normality, but can be more powerful under non-normal distributions. It is often preferred over the $t$-test when the normality assumption cannot be justified. 
However, it can be equally vulnerable to outliers [@FriGat07rank].

We implemented tests for the outlined situation with the following objectives in mind:

* robustness against outliers,
* (approximately) distribution free, i.e. keep $\alpha$ under $H_0$ over many continuous distributions, and
* have a large power over several distributions.

In this vignette, we describe the basic functionality of the package and the implemented tests.

In addition to the two-sample location problem, the tests can be used to detect scale differences between two independent samples. 
This will be briefly discussed at the end of the vignette.

In the remainder, let $\boldsymbol{x} = \left(x_1, \ldots, x_m\right)$ and $\boldsymbol{y} = \left(y_1, \ldots, y_n\right)$ be observed samples from both distributions.


```r
library(robTests)

sessionInfo()
#> R version 4.0.4 (2021-02-15)
#> Platform: x86_64-pc-linux-gnu (64-bit)
#> Running under: Ubuntu 20.04.2 LTS
#> 
#> Matrix products: default
#> BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0
#> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0
#> 
#> locale:
#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=de_AT.UTF-8       
#>  [4] LC_COLLATE=en_US.UTF-8     LC_MONETARY=de_AT.UTF-8    LC_MESSAGES=en_US.UTF-8   
#>  [7] LC_PAPER=de_AT.UTF-8       LC_NAME=C                  LC_ADDRESS=C              
#> [10] LC_TELEPHONE=C             LC_MEASUREMENT=de_AT.UTF-8 LC_IDENTIFICATION=C       
#> 
#> attached base packages:
#> [1] stats     graphics  grDevices utils     datasets  methods   base     
#> 
#> other attached packages:
#> [1] robTests_0.1.0
#> 
#> loaded via a namespace (and not attached):
#>  [1] gtools_3.8.2      digest_0.6.27     rbibutils_2.0     backports_1.2.1   magrittr_2.0.1   
#>  [6] evaluate_0.14     Rdpack_2.1.1      rlang_0.4.10      stringi_1.5.3     robustbase_0.93-7
#> [11] checkmate_2.0.0   rmarkdown_2.6     tools_4.0.4       stringr_1.4.0     DEoptimR_1.0-8   
#> [16] xfun_0.21         yaml_2.2.1        compiler_4.0.4    htmltools_0.5.1.1 knitr_1.31
```

# Overview of implemented tests

The package contains the following two-sample tests:
  
Function name | Test names | Description | Literature
------------- | --------- | ----------- | ----------
`med_test`    | MED1-test, MED2-test | Tests based on the difference of the sample medians | @FriDeh11robu
`hl1_test`    | HL11-test, HL12-test | Tests based on the one-sample Hodges-Lehmann estimator | @FriDeh11robu
`hl2_test`    | HL21-test, HL22-test | Tests based on the two-sample Hodges-Lehmann estimator | @FriDeh11robu
`m_test`      | M-estimator test | Tests based on M-estimators | e.g. @Abo92robu
`trimmed_test`| Yuen's trimmed t-test | Test based on trimmed means | @YueDix73appr

We describe the test statistics in detail in the next subsection. 
All functions contain several arguments to adjust the functions to certain data situations and objectives.

## Test statistics

The implemented test statistics follow the construction principle of the $t$-statistic, i.e. an estimator $\hat{\Delta}$ for the true location difference $\Delta$ between both samples is divided by a pooled estimator $\hat{S}$ for the unknown standard deviation $\sigma$, leading to test
statistics of the form

\begin{align*}
  T = \frac{\hat{\Delta} - \Delta_0}{\hat{S}}.
\end{align*}

In the $t$-statistic, $\hat{\Delta}$ is estimated by $\overline{x} - \overline{y}$, where $\overline{x}$ and $\overline{y}$ are the sample means of $\boldsymbol{x}$ and $\boldsymbol{y}$, respectively. 
The denominator $\hat{S}$ is the pooled empirical standard deviation. 
We replace both estimators by the robust estimators described in the following subsections.

For each test, the argument `alternative` specifies which hypothesis pair is tested. 
The following table shows the different options for the simplified case $\Delta_0 = 0$.

Value of argument `alternative` | Alternative hypothesis | Meaning of the alternative hypothesis
--------------------- | ------------ | -----------
`two.sided`                     | $H_1^{(=)}\colon\ \Delta \neq 0$ | $X$ and $Y$ are stocastically unequal
`greater`                       | $H_1^{(\leq)}\colon\ \Delta > 0$ | $X$ is stochastically larger than $Y$
`less`                          | $H_1^{(\geq)}\colon\ \Delta < 0$ | $X$ is stochastically smaller than $Y$

### MED-tests

Using the difference of the sample medians, i.e.

\begin{align*}
   \hat{\Delta}^{(\text{MED})} = \text{median}\left(x_1, \ldots, x_m\right) - \text{median}\left(y_1, \ldots, y_n\right)
\end{align*}

is an obvious way to estimate the location difference $\Delta$ robustly.

Tests based on this estimator can be called with the function `med_test`.

Two different options for estimating the within-sample variability are available. 
Setting the argument `scale = S3`, it is estimated by the median of the absolute deviations of each observation from its corresponding sample median, namely

\begin{align*}
  \hat{S}^{(3)} = 2 \cdot \text{median}\left\{|x_1 - \tilde{x}|, \ldots, |x_m - \tilde{x}|, |y_1 - \tilde{y}|, \ldots, |y_n - \tilde{y}|\right\},
\end{align*}

where $\tilde{x}$ and $\tilde{y}$ are the sample medians.

Another possibility is `scale = S4` to estimate the scale by the sum of the median absolute deviations from the sample median (MAD) of each sample, i.e.

\begin{align*}
  \hat{S}^{(4)} = \text{median}\left\{|x_1 - \tilde{x}|, \ldots, |x_m - \tilde{x}|\right\} + \text{median}\left\{|y_1 - \tilde{y}|, \ldots, |y_n - \tilde{y}|\right\}.
\end{align*}

<!-- \begin{align*} -->
<!--   \hat{S}^{(4)} = 1.4826 \cdot \text{median}\left\{|x_1 - \tilde{x}|, \ldots, |x_m - \tilde{x}|\right\} + 1.4826 \cdot \text{median}\left\{|y_1 - \tilde{y}|, \ldots, |y_n - \tilde{y}|\right\}. -->
<!-- \end{align*} -->

<!-- The constant `1.4826` allows for an unbiased estimation of the sample standard deviation by the MAD under normality. -->

### HL1-tests

A disadvantage of the sample median is its low efficiency under normality compared to the sample mean. 
Estimators that provide a compromise between robustness and efficiency are the Hodges-Lehmann estimators [@HodLeh63esti].

An estimator for the location difference based on the one-sample Hodges-Lehmann estimator (HL1-estimator) is given by

\begin{align*}
 \hat{\Delta}^{(\text{HL1})} = \text{median}\left\{\frac{x_i + x_j}{2}\colon\ 1 \leq i < j \leq m\right\} - \text{median}\left\{\frac{y_i + y_j}{2}\colon\ 1 \leq i < j \leq n\right\}.
\end{align*}

The test is performed by the function `hl1_test`.

By setting `scale = S1`, the within-sample variability is estimated by the median of the absolute pairwise differences within both samples:

\begin{align*}
  \hat{S}^{(1)} = \text{median}\left\{|x_i - x_j|\colon\ 1 \leq i < j \leq m,\ |y_i - y_j|\colon\ 1 \leq i < j \leq n\right\}.
\end{align*}

Using `scale = S2` estimates it by the absolute pairwise differences within the joint sample, where every observation is centred by its corresponding sample median:

\begin{align*}
  \hat{S}^{(2)} = \text{median}\left\{|z_i - z_j|\colon\ 1 \leq i < j \leq m + n\right\},
\end{align*}

with

\begin{align*}
  \left(z_1, \ldots, z_{m + n}\right) = \left(x_1 - \tilde{x}, \ldots, x_m - \tilde{x}, y_1 - \tilde{y}, \ldots, y_n - \tilde{y}\right).
\end{align*}

### HL2-tests

Instead of subtracting location estimates of both samples, the two-sample Hodges-Lehmann estimator (HL2-estimator) estimates the location difference directly:

\begin{align*}
   \hat{\Delta}^{(\text{HL2})} = \text{median}\left\{|x_i - y_j|\colon\ 1 \leq i \leq m,\ 1 \leq j \leq n\right\}.
\end{align*}

The test is performed by the function `hl2_test` with the same scale estimators as for the HL1-test.

### M-tests

Very flexible location estimators can be derived via M-estimation.
An M-estimator $\hat{\mu}^{(M)}_X$ for the location parameter of the $x$-sample (and analogously for the $y$-sample) can be obtained by solving the minimization problem

\begin{equation*}
  \hat{\mu}^{(M)}_X = \arg\underset{\mu_X}{\min} \sum_{i = 1}^m \rho\left(\frac{x_i - \mu_X}{\sigma_X}\right),
\end{equation*}

where $\rho:\ \mathbb{R} \to \mathbb{R}$ is a function chosen to achieve a \enquote{nearly optimal} location estimate when the $X_i$ are normally or approximately normally distributed [@MarMarYoh06robu, p. 22].
For differentiable functions $\rho$ with $\rho' = \psi$, the optimization problem can be translated to finding the value of $\mu_X$, for which

\begin{equation*}
  \sum_{i = 1}^m \psi\left(\frac{x_i - \mu_X}{\sigma_X}\right) \overset{!}{=} 0.
\end{equation*}

If $\psi$ is an increasing function, it can be shown that

\begin{equation*}
  \sqrt{m} \left(\hat{\mu}^{(M)}_X - \mu_X\right) \overset{\text{asympt.}}{\sim} \mathcal{N}\left(0, \sigma_X^2 \cdot \nu_X\right),
\end{equation*}

with

\begin{equation*}
  \nu_X = \frac{\text{E}\left(\psi\left(\frac{X - \mu_X}{\sigma_X}\right)^2\right)}{\left(\text{E}\left(\psi\left(\frac{X - \mu_X}{\sigma_X}\right)\right)\right)^2}.
\end{equation*}

We estimate $\nu_X$ by replacing the expected values by the sample means, i.e.

\begin{equation*}
\hat{\nu}_X = \frac{\frac{1}{m} \sum_{i = 1}^m \psi\left(\frac{x_i - \hat{\mu}^{(M)}_X}{\sigma_X}\right)^2}{\left(\frac{1}{m} \sum_{i = 1}^m \psi\left(\frac{x_i - \hat{\mu}^{(M)}_X}{\sigma_X}\right)\right)^2}.
\end{equation*}

The only estimator $\hat{\sigma}_X$ for $\sigma_X$ that ensures the asymptotical normality stated before is the empirical standard deviation, which, however, is not robust.
In our package, we use the $\tau$-scale estimator as implemented in the package [robustbase](https://cran.r-project.org/package=robustbase) [@MaeRouCro20robu].
The quality of the normal approximation depends on the true data-generating distribution. 
In some simulations, we found that the approximation works quite well for symmetrical distributions as long as the variance exists.
However, it becomes worse for very skewed distributions, under which the resulting two-sample tests become conservative.
Larger samples are necessary for an acceptable approximation.

The test statistic of the two-sample $M$-estimator test in the package is
\begin{equation*}
  \sqrt{\frac{m \cdot n}{n \cdot \hat{\sigma}^2_X \cdot \hat{\nu}_X + m \cdot \hat{\sigma}^2_Y \cdot \hat{\nu}_Y}} \cdot \left( \hat{\mu}^{(M)}_X - \hat{\mu}^{(M)}_Y \right).
\end{equation*}

We currently allow for Huber-, Hampel-, and Bisquare $\psi$-functions called from [robustbase](https://cran.r-project.org/package=robustbase).

### Trimmed t-test

A standard robust two-sample test is the trimmed t-Test as proposed in @YueDix73appr. The test statistic is based on trimmed means and winsorized variances given as follows:

\begin{align}
  t_{\text{trim}} = \frac{\bar{x}_{m,\gamma} - \bar{y}_{n,\gamma}}{\sqrt{\frac{(m - 1) s^2_{x,\gamma} + (n - 1) s^2_{y, \gamma}}{h.x + h.y - 2}}}
\end{align}

where, for $k_x = \lfloor \gamma m\rfloor$ and $x_{(1)},...,x_{(m)}$ the ordered sample, the trimmed mean is given by:
\begin{align}
  \bar{x}_{m,\gamma} = \frac{1}{m - k_x} \sum_{i=k_x+1}^{m-k_x} x_{(i)} , \quad \gamma\in[0, 1].
\end{align}
The winsorized variance is obtained by replacing the smallest and largest $k_x$ observations by $x_{(k_x)}$ and $x_{(m-k_x)}$, obtaining the modified sample $\tilde{x}_1,...,\tilde{x}_m$. Then:
\begin{align}
  s_{x,\gamma} = \frac{1}{m-1}  \sum_{i=1}^{m} (\tilde{x}_i - \bar{\tilde{x}})^2
\end{align}
(analogously for $y$). $h_x$ denotes the number of samples used, i.e.\ $h_x = m - 2k_x$.

## Computation of p-values

In this section, we describe briefly how the $p$-values of the tests are computed.

The argument `method` can be set to `"permutation"` for a permutation test, 
`"randomization"` for a randomization test, and `"asymptotic"` for an asymptotic test.

### Permutation test

When using the permutation principle, the $p$-value is obtained by computing the value of the test statistic for all possible $B = \binom{m}{m + n}$ splits of the joint sample $\left(x_1, \ldots, x_m, y_1, \ldots, y_n\right)$ into two samples of sizes $m$ and $n$.
The underlying idea is that all splits occur with equal probability under $H_0$. 

The permutation principle allows for an exact computation of the $p$-value and leads to distribution-free tests, i.e. a permutation test keeps $\alpha$ under every continuous distribution.

The $p$-value corresponds to the fraction of computed values of the test statistic, which are at least as extreme as the observed value.
Let $A$ be the number of theses values, $t_i$ the value of the test statistic for split $i$, $i = 1, \ldots, B$, and $t^{(\text{obs})}$ the observed value of the test statistic. Then,

* for $H_0^{(=)}$: $A = \#\left(|t_i| \geq |t^{(\text{obs})|}\right)$
* for $H_0^{(\leq)}$: $A = \#\left(t_i \geq t^{(\text{obs})}\right)$
* for $H_0^{(\geq)}$: $A = \#\left(t_i \leq t^{(\text{obs})}\right)$

and $p$-value $= \frac{A}{B}$.

$B$ increases rapidly in $m$ and $n$, so using the permutation principle can lead to memory and computation time issues.
We recommend this approach only for very small sample sizes or when sufficient computational resources are available.

### Randomization

The randomization principle can be used to deal with the aforementioned computational shortcomings of the permutation principle. 
Instead of computing all possible splits, only a random subset of $b \ll B$ splits is selected. 
As commonly proposed in the literature (e.g. [@PhiSmy10perm]), we draw these random subsets with replacement, i.e. a single permutation may be drawn repeatedly.

The $p$-value can then be estimated by $\frac{A + 1}{b + 1}$, where $A$ now relates to the drawn subsets of the splits. 
In the numerator and the denominator, the number $1$ is added as the observed samples are also a legitimate split.

This estimator will overestimate the true $p$-value because the observed sample may be drawn several times. 
Following [@PhiSmy10perm], we correct the $p$-value using the function `permp` from their R package [statmod](https://cran.r-project.org/package=statmod).

### Asymptotic

For large sample sizes ($m,n>30$), asymptotic $p$-values can be computed for each test using a normal approximation. This reduces the computation time further as compared to the randomization principle.

#### Asymptotic MED-test

Using the asymptotic distribution of the sample median, the test statistic of the asymptotic MED-test is

\begin{align*}
  T_a^{(\text{MED})} = \sqrt{\frac{m \cdot n}{m + n}} \cdot 2 \cdot f\left(F^{-1}\left(0.5\right)\right) \cdot  \hat{\Delta}^{(\text{MED})} \overset{\text{asympt.}}{\sim} \mathcal{N}\left(0, 1\right),
\end{align*}

where $f$ is the density function belonging to the cumulative distribution function $F$. The value $f\left(F^{-1}\left(0.5\right)\right)$ can be estimated by a kernel-density estimator on the sample $x_1 - \tilde{x}, \ldots, x_m - \tilde{x}, y_1 - \tilde{y}, \ldots, y_n - \tilde{y}$. 
We use the R function `density` from the [stats](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html) package with its default settings for the kernel-density estimation.

#### Asymptotic HL1-/HL2-test

With $\lambda = \frac{m}{m + n} \in \left(0, 1\right)$, the asymptotic HL2-test has the test statistic

\begin{align*}
T_a^{(\text{HL2})} = \sqrt{12 \cdot \lambda \cdot \left(1 - \lambda\right)} \int_{-\infty}^{\infty} f^2\left(x\right) \mathrm{d}x \cdot \left(m + n\right) \cdot \hat{\Delta}^{(\text{HL2})} \overset{\text{asympt.}}{\sim} \mathcal{N}\left(0, 1\right),
\end{align*}

where $\int_{-\infty}^{\infty} f^2\left(x\right) \mathrm{d}x$ can be interpreted as the value of the density of $X - Y$ at zero. 
In practice, it can be estimated by a kernel-density estimator on the within-sample pairwise differences $x_2 - x_1, \ldots, x_m - x_1, \ldots, x_m - x_{m - 1}, y_2 - y_1, \ldots, y_n - y_1, \ldots, y_n - y_{n - 1}$.

Replacing $\hat{\Delta}^{(\text{HL2})}$ by $\hat{\Delta}^{(\text{HL1})}$ gives us the test statistic of the asymptotic HL1-test.
[@FriDeh11robu] recommend the asymptotic tests for sample sizes $m, n \geq 30$ to hold the significance level $\alpha = 0.05$. 

#### Asymptotic M-test

As stated before, the test statistic of the two-sample $M$-test follows an asymptotic normal distribution under certain assumptions.
However, as we estimate some parameters, this is only valid approximately and greatly depends on the data-generating distribution.

#### Asymptotic trimmed t-test

The trimmed t-statistic is compared to the respective quantile of the $t_{h_x + h_y - 2}$ distribution. For details see @YueDix73appr.

# Testing for a location difference

We will now show how to use the functions to test for location difference between two samples.

## Continuous data

Let $x_1, \ldots, x_m$ and $y_1, \ldots, y_m$ be observations from a continuous distributions.


```r
set.seed(108)
x <- rnorm(10)
y <- rnorm(10)

x
#>  [1] -0.112892122 -0.381819373 -0.091691759  0.036045206  0.002060011  1.254923407  0.589883973
#>  [8]  0.302208595  0.905741048  0.054724887
y
#>  [1] -0.8130825  1.3807136 -0.4387136 -1.2788213 -0.6058519  0.9234760  0.5656956 -0.0586360  0.1578270
#> [10] -0.3858708
```

### Permutation test

In this example, we use the two-sided HL1-test with the scale estimator $\hat{S}^{(2)}$ and
assume that $\Delta_0 = 0$.


```r
hl1_test(x = x, y = y, alternative = "two.sided", delta = 0, method = "permutation", scale = "S2")
#> 
#> 	Exact permutation test based on the one-sample Hodges-Lehmann estimator
#> 
#> data:  x and y
#> D = 0.55524, p-value = 0.27
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>   HL1 of x   HL1 of y 
#>  0.2384959 -0.1140219
```

The output is an object of the class `htest` and contains all relevant information on the performed test.
The $p$-value can be accessed via `hl1.res$p.value`. 
To extract the value of the test statistic, we use `hl1.res$statistic`, and for the HL1-estimates of each sample `hl1.res$estimate`.

### Randomization test

We draw 10000 splits randomly from the observed joint sample to use the randomization principle on the HL1-test.


```r
set.seed(47)

hl1_test(x = x, y = y, alternative = "two.sided", delta = 0, method = "randomization", scale = "S2", n.rep = 10000)
#> 
#> 	Randomization test based on the one-sample Hodges-Lehmann estimator using 10000 random
#> 	permutations
#> 
#> data:  x and y
#> D = 0.55524, p-value = 0.2757
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>   HL1 of x   HL1 of y 
#>  0.2384959 -0.1140219
```

The $p$-value is close to the one we have calculated with the permutation principle.

### Asymptotic test
Although the sample sizes in our example are rather small, we use the samples to demonstrate how to perform an asymptotic HL1-test.


```r
hl1_test(x = x, y = y, alternative = "two.sided", delta = 0, method = "asymptotic")
#> 
#> 	Asymptotic test based on the one-sample Hodges-Lehmann estimator
#> 
#> data:  x and y
#> D = 1.0366, p-value = 0.2999
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>   HL1 of x   HL1 of y 
#>  0.2384959 -0.1140219
```

The $p$-value is not too far away from those obtained by the permutation and randomization principles.

### Location difference under $H_0$

We can also perform the test when assuming a certain location difference between both samples under $H_0$, i.e. $\Delta_0 \neq 0$.
In this example, we use $\Delta_0 = 5$, i.e. under $H_0$, we assume that the location parameter of the distribution of $X_i$, $i = 1, \ldots, m$, is five units larger than that of the distribution of $Y_j$, $j = 1, \ldots, n$.
We shift $x_1, \ldots, x_m$ by five units so that $H_0$ should not be rejected.


```r
x1 <- x + 5

hl1_test(x = x1, y = y, alternative = "two.sided", delta = 5, method = "asymptotic")
#> 
#> 	Asymptotic test based on the one-sample Hodges-Lehmann estimator
#> 
#> data:  x1 and y
#> D = 1.0366, p-value = 0.2999
#> alternative hypothesis: true location shift is not equal to 5
#> sample estimates:
#>   HL1 of x   HL1 of y 
#>  5.2384959 -0.1140219
```

As expected, the $p$-value is the same as the one from the previous test. 

## Discrete data

In many applications the data may be rounded to a small number of digits or the data-generating process may be discrete.
This can lead to several problems:

* If there is no location difference, the scale estimate, i.e. the denominator of the test statistic, may be zero so that the test statistic cannot be computed.
* The test has only little power, i.e. location differences between the samples may not be found.

Following the suggestion of [@FriGat07rank], we implemented a procedure called wobbling, where we add random noise from a uniform distribution to the observations. 
To keep the alterations small, the scale of the noise depends on the number of decimal places of the observations.
This is controlled by the argument `wobble`, where `TRUE` means that noise is added and `FALSE` means that no noise is added.

In the following example, we round the previously generated observations and perform the HL12-test.


```r
x1 <- round(x)
y1 <- round(y)

x1
#>  [1] 0 0 0 0 0 1 1 0 1 0
y1
#>  [1] -1  1  0 -1 -1  1  1  0  0  0

set.seed(47)
hl1_test(x1, y1, alternative = "two.sided", method = "randomization", scale = "S2")
#> Error in rob_var(x, y, type = "S2"): Estimate of scale is '0' although the data are not constant.
#>           Consider using a different estimator or setting wobble = TRUE in the function call. Otherwise, the test statistic cannot be computed.
```

Although the value of the scale estimator in the original sample is `rob_var(x1, y1, type = "S2")`, there exists at least one split used to compute the randomization distribution, which leads to the estimated scale `0`.
We follow the advice in the error message and set `wobble = TRUE`. To enable the reproducibility of the results, the argument `wobble.seed` can be set, otherwise a random seed is chosen.


```r
set.seed(47)
hl1_test(x1, y1, alternative = "two.sided", method = "randomization", scale = "S2", wobble = TRUE, wobble.seed = 2187)
#> Warning in preprocess_data(x = x, y = y, delta = delta, na.rm = na.rm, wobble = wobble, : Added random
#> noise to x and y. The seed is 2187.
#> 
#> 	Randomization test based on the one-sample Hodges-Lehmann estimator using 10000 random
#> 	permutations
#> 
#> data:  x1 and y1
#> D = 0.15486, p-value = 0.7359
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>   HL1 of x   HL1 of y 
#> 0.21979333 0.08923926
```

The $p$-value deviates quite strongly from the $p$-value obtained for the original observations.
This is because we cannot reproduce them exactly.
If we chose to wobble on the original observations, the $p$-value would again be quite close to the one for the unrounded values.


```r
set.seed(47)
hl1_test(x, y, alternative = "two.sided", method = "randomization", scale = "S2", wobble = TRUE, wobble.seed = 2187)
#> 
#> 	Randomization test based on the one-sample Hodges-Lehmann estimator using 10000 random
#> 	permutations
#> 
#> data:  x and y
#> D = 0.55524, p-value = 0.2757
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>   HL1 of x   HL1 of y 
#>  0.2384959 -0.1140219
```

We now consider an example, where a location difference of size $\Delta = 1.5$ exists between the samples.


```r
y <- y + 1.5

x1 <- trunc(x)
y1 <- trunc(y)

## HL12-test on original observations
set.seed(47)
hl1_test(x, y, alternative = "two.sided", method = "randomization", scale = "S2")
#> 
#> 	Randomization test based on the one-sample Hodges-Lehmann estimator using 10000 random
#> 	permutations
#> 
#> data:  x and y
#> D = -1.8074, p-value = 0.002394
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>  HL1 of x  HL1 of y 
#> 0.2384959 1.3859781

## HL12-test on truncated observations
set.seed(47)
hl1_test(x1, y1, alternative = "two.sided", method = "randomization", scale = "S2")
#> 
#> 	Randomization test based on the one-sample Hodges-Lehmann estimator using 10000 random
#> 	permutations
#> 
#> data:  x1 and y1
#> D = -1, p-value = 0.2482
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#> HL1 of x HL1 of y 
#>        0        1

## HL12-test on truncated observations with wobbling
set.seed(47)
hl1_test(x1, y1, alternative = "two.sided", method = "randomization", scale = "S2", wobble = TRUE, wobble.seed = 2187)
#> Warning in preprocess_data(x = x, y = y, delta = delta, na.rm = na.rm, wobble = wobble, : Added random
#> noise to x and y. The seed is 2187.
#> 
#> 	Randomization test based on the one-sample Hodges-Lehmann estimator using 10000 random
#> 	permutations
#> 
#> data:  x1 and y1
#> D = -1.6304, p-value = 0.005194
#> alternative hypothesis: true location shift is not equal to 0
#> sample estimates:
#>   HL1 of x   HL1 of y 
#> 0.01874928 1.08923926
```

The wobbled samples can be retrieved using the seed printed in the output and the function `wobble`

```r
set.seed(2187)
wobble(x1, y1, check = FALSE)
#> $x
#>  [1] -0.44559409  0.25924234  0.18034432 -0.42507561 -0.07607574  0.68780214  0.17812437 -0.22174378
#>  [9]  0.19595295 -0.14095250
#> 
#> $y
#>  [1] -0.04372255  2.22220106  1.23878909  0.16253680 -0.13902692  2.43266503  1.86621893  1.41821762
#>  [9]  0.71976748  1.36813320
```

# Testing for a difference in scale

Following [@Fri12onli] the argument `var.test` allows to decide whether the samples should be tested for a location difference (`var.test = FALSE`), which is the default, or for different variances (`var.test = TRUE`), i.e. $\sigma^2_X \neq \sigma^2_Y$.

Setting `var.test = TRUE` transforms the observations so that a possible variance difference between the samples appears as a location difference between the transformed samples.

In terms of the power, the resulting variance tests can outperform classical tests like the $F$-test, the Mood test, or the Ansary-Bradley test under outliers and asymmetry.

## Idea
Again, we start with two samples $X_1, \ldots, X_m$ and $Y_1, \ldots, Y_n$, each of which consists of i.i.d. random variables. 
However, now we assume that we can write the random variables as

\begin{align*}
  X_i = \sigma_X \cdot \varepsilon_{X, i},\ i = 1, \ldots, m \quad \text{and} \quad Y_j = \sigma_Y \cdot \varepsilon_{Y, j},\ j = 1, \ldots, n.
\end{align*}

Here, $\sigma_X$ is the standard deviation of the first sample and $\sigma_Y$ is the standard deviation of the second sample. The random variables $\varepsilon_{X, 1}, \ldots, \varepsilon_{X, m}$ and $\varepsilon_{Y, 1}, \ldots, \varepsilon_{Y, m}$ are i.i.d. random variables with expectation zero and unit variance.

We address the following hypotheses pairs:
\begin{align*}
  &H_0^{(=)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} = 1 \quad \text{vs.} \quad H_1^{(\neq)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} \neq 1 \\
  &H_0^{(>)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} \leq 1 \quad \text{vs.} \quad H_1^{(>)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} > 1 \\
  &H_0^{(<)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} \geq 1 \quad \text{vs.} \quad H_1^{(<)}\colon\ \frac{\sigma^2_X}{\sigma^2_Y} < 1.
\end{align*}

By log-transforming the random variables, we obtain

\begin{align*}
  & U_i = \log\left(X_i^2\right) = \log\left(\sigma_X^2\right) + \log\left(\varepsilon_{X, i}^2\right),\ i = 1, \ldots, m, \\ \text{and } & \quad V_i = \log\left(Y_j^2\right) = \log\left(\sigma_Y^2\right) + \log\left(\varepsilon_{Y, j}^2\right), j = 1, \ldots, n.
\end{align*}

With these transformed samples, we are in the data situation described in the introduction as the distributions differ at most by $\Delta = \log\left(\sigma_X^2\right) - \log\left(\sigma_Y^2\right)$.


```r
set.seed(108)
  
x <- rnorm(30)
y <- rnorm(30)

## Asymptotic two-sided test assuming no scale difference between both samples
hl1_test(x = x, y = y, alternative = "two.sided", delta = 1, method = "asymptotic", var.test = TRUE)
#> 
#> 	Asymptotic test based on the one-sample Hodges-Lehmann estimator
#> 
#> data:  x and y
#> S = -0.95431, p-value = 0.3399
#> alternative hypothesis: true ratio of variances is not equal to 1
#> sample estimates:
#> HL1 of log(x^2) HL1 of log(y^2) 
#>       -1.731838       -1.215280
```

Including a scale difference of 5 yields

```r
## Asymptotic two-sided test assuming a scale difference of 5 between both samples
hl1_test(x = x * 5, y = y, alternative = "two.sided", delta = 5, method = "asymptotic", var.test = TRUE)
#> 
#> 	Asymptotic test based on the one-sample Hodges-Lehmann estimator
#> 
#> data:  x * 5 and y
#> S = -0.95431, p-value = 0.3399
#> alternative hypothesis: true ratio of variances is not equal to 25
#> sample estimates:
#> HL1 of log(x^2) HL1 of log(y^2) 
#>        1.487038       -1.215280
```

## Other packages
The tests in this package are constructed assuming homoscedasticity. Robust two-sample tests for the heteroscedastic scenario can be found in R package [WRS2](https://cran.r-project.org/package=WRS2) [@MaiWil20robu].

## References

